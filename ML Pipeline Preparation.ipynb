{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,label_ranking_average_precision_score\n",
    "from sklearn.model_selection  import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponsePipeline.db')\n",
    "df = pd.read_sql_table(\"SELECT * from DisasterResponse\", con=engine)\n",
    "X = df['message']\n",
    "y = df.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function: tokenize the text\n",
    "    Args:  source string\n",
    "    Return:\n",
    "    clean_tokens(str list): clean string list\n",
    "    \n",
    "    \"\"\"\n",
    "    #normalize text\n",
    "    text = re.sub(r'[^a-zA-Z0-9]',' ',text.lower())\n",
    "    \n",
    "    #token messages\n",
    "    words = word_tokenize(text)\n",
    "    tokens = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    #sterm and lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipleine 1: Random Forest Classifier\n",
    "\n",
    "pipeline_RF = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf',  MultiOutputClassifier(RandomForestClassifier(class_weight='balanced')))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Pipleine 2: Adaboost Classifier \n",
    "\n",
    "pipeline_ADA = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf',  MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RF Classifier\n",
    "pipeline_RF.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the Ada Classifier \n",
    "pipeline_ADA.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall results\n",
    "def get_results(Y_test, model):\n",
    "    y_pred=model.predict(X_test)\n",
    "    report = pd.DataFrame(columns=['Category', 'f_score', 'precision', 'recall'])\n",
    "    num = 0\n",
    "    for colnm in Y_test.columns:\n",
    "        precision, recall, f_score, support = precision_recall_fscore_support(Y_test[colnm], y_pred[:,num], average='weighted')\n",
    "        report.at[num+1, 'Category'] = colnm\n",
    "        report.at[num+1, 'f_score'] = f_score\n",
    "        report.at[num+1, 'precision'] = precision\n",
    "        report.at[num+1, 'recall'] = recall\n",
    "        num += 1\n",
    "    print('Aggregated f_score:', report['f_score'].mean())\n",
    "    print('Aggregated precision:', report['precision'].mean())\n",
    "    print('Aggregated recall:', report['recall'].mean())\n",
    "    print('Accuracy:', np.mean(Y_test.values == y_pred))\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated f_score: 0.931343744767\n",
      "Aggregated precision: 0.933839841309\n",
      "Aggregated recall: 0.943685102776\n",
      "Accuracy: 0.943685102776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>f_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>related</td>\n",
       "      <td>0.793613</td>\n",
       "      <td>0.792682</td>\n",
       "      <td>0.801144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>request</td>\n",
       "      <td>0.879544</td>\n",
       "      <td>0.883485</td>\n",
       "      <td>0.890782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer</td>\n",
       "      <td>0.993139</td>\n",
       "      <td>0.990866</td>\n",
       "      <td>0.995423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aid_related</td>\n",
       "      <td>0.746872</td>\n",
       "      <td>0.748143</td>\n",
       "      <td>0.750286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical_help</td>\n",
       "      <td>0.897282</td>\n",
       "      <td>0.905043</td>\n",
       "      <td>0.923204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical_products</td>\n",
       "      <td>0.92835</td>\n",
       "      <td>0.935951</td>\n",
       "      <td>0.94876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>search_and_rescue</td>\n",
       "      <td>0.964094</td>\n",
       "      <td>0.968286</td>\n",
       "      <td>0.975715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>security</td>\n",
       "      <td>0.97243</td>\n",
       "      <td>0.96371</td>\n",
       "      <td>0.98131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>military</td>\n",
       "      <td>0.956276</td>\n",
       "      <td>0.955635</td>\n",
       "      <td>0.968849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>child_alone</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>water</td>\n",
       "      <td>0.943384</td>\n",
       "      <td>0.944191</td>\n",
       "      <td>0.951303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>food</td>\n",
       "      <td>0.913051</td>\n",
       "      <td>0.920165</td>\n",
       "      <td>0.925111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shelter</td>\n",
       "      <td>0.905149</td>\n",
       "      <td>0.913844</td>\n",
       "      <td>0.924348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clothing</td>\n",
       "      <td>0.980408</td>\n",
       "      <td>0.981474</td>\n",
       "      <td>0.985887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>money</td>\n",
       "      <td>0.968662</td>\n",
       "      <td>0.968522</td>\n",
       "      <td>0.978385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>missing_people</td>\n",
       "      <td>0.982803</td>\n",
       "      <td>0.988564</td>\n",
       "      <td>0.98843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>refugees</td>\n",
       "      <td>0.950941</td>\n",
       "      <td>0.951621</td>\n",
       "      <td>0.966942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>death</td>\n",
       "      <td>0.937311</td>\n",
       "      <td>0.946737</td>\n",
       "      <td>0.955499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other_aid</td>\n",
       "      <td>0.825045</td>\n",
       "      <td>0.831854</td>\n",
       "      <td>0.870057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>infrastructure_related</td>\n",
       "      <td>0.905364</td>\n",
       "      <td>0.90008</td>\n",
       "      <td>0.935537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>transport</td>\n",
       "      <td>0.937932</td>\n",
       "      <td>0.949207</td>\n",
       "      <td>0.956135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>buildings</td>\n",
       "      <td>0.934091</td>\n",
       "      <td>0.942239</td>\n",
       "      <td>0.952575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>electricity</td>\n",
       "      <td>0.969849</td>\n",
       "      <td>0.976773</td>\n",
       "      <td>0.979275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tools</td>\n",
       "      <td>0.990791</td>\n",
       "      <td>0.98783</td>\n",
       "      <td>0.99377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hospitals</td>\n",
       "      <td>0.985149</td>\n",
       "      <td>0.980264</td>\n",
       "      <td>0.990083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>shops</td>\n",
       "      <td>0.994663</td>\n",
       "      <td>0.992893</td>\n",
       "      <td>0.99644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aid_centers</td>\n",
       "      <td>0.980336</td>\n",
       "      <td>0.973978</td>\n",
       "      <td>0.986777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>other_infrastructure</td>\n",
       "      <td>0.935943</td>\n",
       "      <td>0.937067</td>\n",
       "      <td>0.956643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>weather_related</td>\n",
       "      <td>0.841973</td>\n",
       "      <td>0.843459</td>\n",
       "      <td>0.848188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>floods</td>\n",
       "      <td>0.925051</td>\n",
       "      <td>0.935062</td>\n",
       "      <td>0.939479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>storm</td>\n",
       "      <td>0.921408</td>\n",
       "      <td>0.921763</td>\n",
       "      <td>0.930451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fire</td>\n",
       "      <td>0.984135</td>\n",
       "      <td>0.979004</td>\n",
       "      <td>0.98932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.958084</td>\n",
       "      <td>0.95864</td>\n",
       "      <td>0.960712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cold</td>\n",
       "      <td>0.971537</td>\n",
       "      <td>0.975112</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>other_weather</td>\n",
       "      <td>0.927465</td>\n",
       "      <td>0.940659</td>\n",
       "      <td>0.949142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>direct_report</td>\n",
       "      <td>0.826251</td>\n",
       "      <td>0.833433</td>\n",
       "      <td>0.847171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Category   f_score precision    recall\n",
       "1                  related  0.793613  0.792682  0.801144\n",
       "2                  request  0.879544  0.883485  0.890782\n",
       "3                    offer  0.993139  0.990866  0.995423\n",
       "4              aid_related  0.746872  0.748143  0.750286\n",
       "5             medical_help  0.897282  0.905043  0.923204\n",
       "6         medical_products   0.92835  0.935951   0.94876\n",
       "7        search_and_rescue  0.964094  0.968286  0.975715\n",
       "8                 security   0.97243   0.96371   0.98131\n",
       "9                 military  0.956276  0.955635  0.968849\n",
       "10             child_alone         1         1         1\n",
       "11                   water  0.943384  0.944191  0.951303\n",
       "12                    food  0.913051  0.920165  0.925111\n",
       "13                 shelter  0.905149  0.913844  0.924348\n",
       "14                clothing  0.980408  0.981474  0.985887\n",
       "15                   money  0.968662  0.968522  0.978385\n",
       "16          missing_people  0.982803  0.988564   0.98843\n",
       "17                refugees  0.950941  0.951621  0.966942\n",
       "18                   death  0.937311  0.946737  0.955499\n",
       "19               other_aid  0.825045  0.831854  0.870057\n",
       "20  infrastructure_related  0.905364   0.90008  0.935537\n",
       "21               transport  0.937932  0.949207  0.956135\n",
       "22               buildings  0.934091  0.942239  0.952575\n",
       "23             electricity  0.969849  0.976773  0.979275\n",
       "24                   tools  0.990791   0.98783   0.99377\n",
       "25               hospitals  0.985149  0.980264  0.990083\n",
       "26                   shops  0.994663  0.992893   0.99644\n",
       "27             aid_centers  0.980336  0.973978  0.986777\n",
       "28    other_infrastructure  0.935943  0.937067  0.956643\n",
       "29         weather_related  0.841973  0.843459  0.848188\n",
       "30                  floods  0.925051  0.935062  0.939479\n",
       "31                   storm  0.921408  0.921763  0.930451\n",
       "32                    fire  0.984135  0.979004   0.98932\n",
       "33              earthquake  0.958084   0.95864  0.960712\n",
       "34                    cold  0.971537  0.975112   0.97953\n",
       "35           other_weather  0.927465  0.940659  0.949142\n",
       "36           direct_report  0.826251  0.833433  0.847171"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get test results or Random Forest\n",
    "get_results(Y_test,pipeline_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated f_score: 0.93845484252\n",
      "Aggregated precision: 0.93855634888\n",
      "Aggregated recall: 0.946680087589\n",
      "Accuracy: 0.946680087589\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>f_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>related</td>\n",
       "      <td>0.69837</td>\n",
       "      <td>0.73264</td>\n",
       "      <td>0.765035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>request</td>\n",
       "      <td>0.877397</td>\n",
       "      <td>0.878533</td>\n",
       "      <td>0.887222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer</td>\n",
       "      <td>0.992694</td>\n",
       "      <td>0.991368</td>\n",
       "      <td>0.994151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aid_related</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.757431</td>\n",
       "      <td>0.757788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical_help</td>\n",
       "      <td>0.912996</td>\n",
       "      <td>0.910866</td>\n",
       "      <td>0.926001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical_products</td>\n",
       "      <td>0.947083</td>\n",
       "      <td>0.946362</td>\n",
       "      <td>0.954863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>search_and_rescue</td>\n",
       "      <td>0.970578</td>\n",
       "      <td>0.969672</td>\n",
       "      <td>0.976605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>security</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.967672</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>military</td>\n",
       "      <td>0.966625</td>\n",
       "      <td>0.964936</td>\n",
       "      <td>0.971011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>child_alone</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>water</td>\n",
       "      <td>0.961899</td>\n",
       "      <td>0.961133</td>\n",
       "      <td>0.963255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>food</td>\n",
       "      <td>0.95039</td>\n",
       "      <td>0.949812</td>\n",
       "      <td>0.951558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shelter</td>\n",
       "      <td>0.941156</td>\n",
       "      <td>0.940419</td>\n",
       "      <td>0.945073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clothing</td>\n",
       "      <td>0.987648</td>\n",
       "      <td>0.987226</td>\n",
       "      <td>0.988811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>money</td>\n",
       "      <td>0.974508</td>\n",
       "      <td>0.972789</td>\n",
       "      <td>0.977114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>missing_people</td>\n",
       "      <td>0.984818</td>\n",
       "      <td>0.984857</td>\n",
       "      <td>0.988684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>refugees</td>\n",
       "      <td>0.965116</td>\n",
       "      <td>0.963732</td>\n",
       "      <td>0.970121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>death</td>\n",
       "      <td>0.960515</td>\n",
       "      <td>0.96078</td>\n",
       "      <td>0.965671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other_aid</td>\n",
       "      <td>0.834096</td>\n",
       "      <td>0.830198</td>\n",
       "      <td>0.867514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>infrastructure_related</td>\n",
       "      <td>0.91351</td>\n",
       "      <td>0.908482</td>\n",
       "      <td>0.934011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>transport</td>\n",
       "      <td>0.947883</td>\n",
       "      <td>0.949581</td>\n",
       "      <td>0.958805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>buildings</td>\n",
       "      <td>0.954966</td>\n",
       "      <td>0.955104</td>\n",
       "      <td>0.961221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>electricity</td>\n",
       "      <td>0.976165</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.980038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tools</td>\n",
       "      <td>0.990088</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.99199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hospitals</td>\n",
       "      <td>0.985174</td>\n",
       "      <td>0.983037</td>\n",
       "      <td>0.987921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>shops</td>\n",
       "      <td>0.994341</td>\n",
       "      <td>0.993371</td>\n",
       "      <td>0.995423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aid_centers</td>\n",
       "      <td>0.981554</td>\n",
       "      <td>0.979276</td>\n",
       "      <td>0.985633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>other_infrastructure</td>\n",
       "      <td>0.941031</td>\n",
       "      <td>0.935664</td>\n",
       "      <td>0.953973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>weather_related</td>\n",
       "      <td>0.871198</td>\n",
       "      <td>0.874661</td>\n",
       "      <td>0.876414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>floods</td>\n",
       "      <td>0.953949</td>\n",
       "      <td>0.954759</td>\n",
       "      <td>0.957788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>storm</td>\n",
       "      <td>0.928534</td>\n",
       "      <td>0.928449</td>\n",
       "      <td>0.93541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fire</td>\n",
       "      <td>0.987915</td>\n",
       "      <td>0.987148</td>\n",
       "      <td>0.989701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.969377</td>\n",
       "      <td>0.969123</td>\n",
       "      <td>0.970121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cold</td>\n",
       "      <td>0.980444</td>\n",
       "      <td>0.980411</td>\n",
       "      <td>0.983217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>other_weather</td>\n",
       "      <td>0.931211</td>\n",
       "      <td>0.925684</td>\n",
       "      <td>0.944056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>direct_report</td>\n",
       "      <td>0.826329</td>\n",
       "      <td>0.829443</td>\n",
       "      <td>0.844755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Category   f_score precision    recall\n",
       "1                  related   0.69837   0.73264  0.765035\n",
       "2                  request  0.877397  0.878533  0.887222\n",
       "3                    offer  0.992694  0.991368  0.994151\n",
       "4              aid_related  0.752212  0.757431  0.757788\n",
       "5             medical_help  0.912996  0.910866  0.926001\n",
       "6         medical_products  0.947083  0.946362  0.954863\n",
       "7        search_and_rescue  0.970578  0.969672  0.976605\n",
       "8                 security  0.972603  0.967672   0.97953\n",
       "9                 military  0.966625  0.964936  0.971011\n",
       "10             child_alone         1         1         1\n",
       "11                   water  0.961899  0.961133  0.963255\n",
       "12                    food   0.95039  0.949812  0.951558\n",
       "13                 shelter  0.941156  0.940419  0.945073\n",
       "14                clothing  0.987648  0.987226  0.988811\n",
       "15                   money  0.974508  0.972789  0.977114\n",
       "16          missing_people  0.984818  0.984857  0.988684\n",
       "17                refugees  0.965116  0.963732  0.970121\n",
       "18                   death  0.960515   0.96078  0.965671\n",
       "19               other_aid  0.834096  0.830198  0.867514\n",
       "20  infrastructure_related   0.91351  0.908482  0.934011\n",
       "21               transport  0.947883  0.949581  0.958805\n",
       "22               buildings  0.954966  0.955104  0.961221\n",
       "23             electricity  0.976165  0.975104  0.980038\n",
       "24                   tools  0.990088  0.988304   0.99199\n",
       "25               hospitals  0.985174  0.983037  0.987921\n",
       "26                   shops  0.994341  0.993371  0.995423\n",
       "27             aid_centers  0.981554  0.979276  0.985633\n",
       "28    other_infrastructure  0.941031  0.935664  0.953973\n",
       "29         weather_related  0.871198  0.874661  0.876414\n",
       "30                  floods  0.953949  0.954759  0.957788\n",
       "31                   storm  0.928534  0.928449   0.93541\n",
       "32                    fire  0.987915  0.987148  0.989701\n",
       "33              earthquake  0.969377  0.969123  0.970121\n",
       "34                    cold  0.980444  0.980411  0.983217\n",
       "35           other_weather  0.931211  0.925684  0.944056\n",
       "36           direct_report  0.826329  0.829443  0.844755"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get result for ADA classifier\n",
    "get_results(Y_test,pipeline_ADA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f2779a1bc80>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "               criterion='gini', max_depth=None, max_features='auto',\n",
       "               max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "               min_impurity_split=None, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "               verbose=0, warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f2779a1bc80>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "             criterion='gini', max_depth=None, max_features='auto',\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': 'balanced',\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "             criterion='gini', max_depth=None, max_features='auto',\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show parameters for the pipline\n",
    "pipeline_RF.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f2779a1bc80>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f2779a1bc80>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'SAMME.R',\n",
       " 'clf__estimator__base_estimator': None,\n",
       " 'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show parameters for the pipline\n",
    "pipeline_ADA.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': [1.0, 2.0], 'tfidf__use_idf': [True, False], 'clf__estimator__criterion': ['gini'], 'clf__estimator__n_estimators': [10, 20], 'clf__estimator__min_samples_split': [2, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_RF = {\n",
    "        'vect__max_df': [1.0,2.0],\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'clf__estimator__criterion': ['gini'],\n",
    "        'clf__estimator__n_estimators': [10,20],\n",
    "        'clf__estimator__min_samples_split':[2,5]\n",
    "        \n",
    "}\n",
    "\n",
    "cv_RF = GridSearchCV(pipeline_RF, param_grid = parameters_RF, verbose=True,cv=3,n_jobs = -1)\n",
    "cv_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': [1.0, 2.0], 'tfidf__use_idf': [True, False], 'clf__estimator__n_estimators': [50, 70], 'clf__estimator__learning_rate': [1, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_ADA = {\n",
    "        'vect__max_df': [1.0,2.0],\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'clf__estimator__n_estimators': [50,70],\n",
    "        'clf__estimator__learning_rate': [1,2]\n",
    "}\n",
    "\n",
    "cv_ADA = GridSearchCV(pipeline_ADA, param_grid = parameters_ADA, verbose=True,cv=3,n_jobs = -1)\n",
    "cv_ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 141.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': [1.0, 2.0], 'tfidf__use_idf': [True, False], 'clf__estimator__criterion': ['gini'], 'clf__estimator__n_estimators': [10, 20], 'clf__estimator__min_samples_split': [2, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the RF model\n",
    "cv_RF.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 163.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': [1.0, 2.0], 'tfidf__use_idf': [True, False], 'clf__estimator__n_estimators': [50, 70], 'clf__estimator__learning_rate': [1, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ADA.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated f_score: 0.939125741999\n",
      "Aggregated precision: 0.93863622071\n",
      "Aggregated recall: 0.947033269761\n",
      "Accuracy: 0.947033269761\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>f_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>related</td>\n",
       "      <td>0.804193</td>\n",
       "      <td>0.803366</td>\n",
       "      <td>0.808646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>request</td>\n",
       "      <td>0.893826</td>\n",
       "      <td>0.892297</td>\n",
       "      <td>0.896631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer</td>\n",
       "      <td>0.993076</td>\n",
       "      <td>0.990866</td>\n",
       "      <td>0.995296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aid_related</td>\n",
       "      <td>0.769707</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.768976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical_help</td>\n",
       "      <td>0.911211</td>\n",
       "      <td>0.907168</td>\n",
       "      <td>0.922441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical_products</td>\n",
       "      <td>0.940025</td>\n",
       "      <td>0.94082</td>\n",
       "      <td>0.951939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>search_and_rescue</td>\n",
       "      <td>0.96569</td>\n",
       "      <td>0.969004</td>\n",
       "      <td>0.976097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>security</td>\n",
       "      <td>0.972367</td>\n",
       "      <td>0.963708</td>\n",
       "      <td>0.981182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>military</td>\n",
       "      <td>0.964139</td>\n",
       "      <td>0.96274</td>\n",
       "      <td>0.970502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>child_alone</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>water</td>\n",
       "      <td>0.953097</td>\n",
       "      <td>0.952442</td>\n",
       "      <td>0.957152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>food</td>\n",
       "      <td>0.942331</td>\n",
       "      <td>0.941765</td>\n",
       "      <td>0.944819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shelter</td>\n",
       "      <td>0.924023</td>\n",
       "      <td>0.923445</td>\n",
       "      <td>0.932231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clothing</td>\n",
       "      <td>0.983101</td>\n",
       "      <td>0.983791</td>\n",
       "      <td>0.986904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>money</td>\n",
       "      <td>0.970185</td>\n",
       "      <td>0.971153</td>\n",
       "      <td>0.978767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>missing_people</td>\n",
       "      <td>0.983112</td>\n",
       "      <td>0.988688</td>\n",
       "      <td>0.988557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>refugees</td>\n",
       "      <td>0.955561</td>\n",
       "      <td>0.955685</td>\n",
       "      <td>0.967451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>death</td>\n",
       "      <td>0.951104</td>\n",
       "      <td>0.956573</td>\n",
       "      <td>0.961602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other_aid</td>\n",
       "      <td>0.84216</td>\n",
       "      <td>0.840074</td>\n",
       "      <td>0.871329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>infrastructure_related</td>\n",
       "      <td>0.909023</td>\n",
       "      <td>0.91061</td>\n",
       "      <td>0.935919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>transport</td>\n",
       "      <td>0.941023</td>\n",
       "      <td>0.94704</td>\n",
       "      <td>0.956771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>buildings</td>\n",
       "      <td>0.945112</td>\n",
       "      <td>0.94529</td>\n",
       "      <td>0.955372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>electricity</td>\n",
       "      <td>0.970561</td>\n",
       "      <td>0.971502</td>\n",
       "      <td>0.979021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tools</td>\n",
       "      <td>0.990791</td>\n",
       "      <td>0.98783</td>\n",
       "      <td>0.99377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hospitals</td>\n",
       "      <td>0.985149</td>\n",
       "      <td>0.980264</td>\n",
       "      <td>0.990083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>shops</td>\n",
       "      <td>0.994663</td>\n",
       "      <td>0.992893</td>\n",
       "      <td>0.99644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aid_centers</td>\n",
       "      <td>0.980336</td>\n",
       "      <td>0.973978</td>\n",
       "      <td>0.986777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>other_infrastructure</td>\n",
       "      <td>0.936331</td>\n",
       "      <td>0.930149</td>\n",
       "      <td>0.956008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>weather_related</td>\n",
       "      <td>0.872235</td>\n",
       "      <td>0.871761</td>\n",
       "      <td>0.872854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>floods</td>\n",
       "      <td>0.935705</td>\n",
       "      <td>0.938453</td>\n",
       "      <td>0.944437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>storm</td>\n",
       "      <td>0.934339</td>\n",
       "      <td>0.932969</td>\n",
       "      <td>0.936427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fire</td>\n",
       "      <td>0.98584</td>\n",
       "      <td>0.987665</td>\n",
       "      <td>0.989955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.968222</td>\n",
       "      <td>0.967977</td>\n",
       "      <td>0.969104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cold</td>\n",
       "      <td>0.971675</td>\n",
       "      <td>0.971777</td>\n",
       "      <td>0.978894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>other_weather</td>\n",
       "      <td>0.932659</td>\n",
       "      <td>0.933459</td>\n",
       "      <td>0.949015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>direct_report</td>\n",
       "      <td>0.835957</td>\n",
       "      <td>0.832709</td>\n",
       "      <td>0.841831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Category   f_score precision    recall\n",
       "1                  related  0.804193  0.803366  0.808646\n",
       "2                  request  0.893826  0.892297  0.896631\n",
       "3                    offer  0.993076  0.990866  0.995296\n",
       "4              aid_related  0.769707  0.770992  0.768976\n",
       "5             medical_help  0.911211  0.907168  0.922441\n",
       "6         medical_products  0.940025   0.94082  0.951939\n",
       "7        search_and_rescue   0.96569  0.969004  0.976097\n",
       "8                 security  0.972367  0.963708  0.981182\n",
       "9                 military  0.964139   0.96274  0.970502\n",
       "10             child_alone         1         1         1\n",
       "11                   water  0.953097  0.952442  0.957152\n",
       "12                    food  0.942331  0.941765  0.944819\n",
       "13                 shelter  0.924023  0.923445  0.932231\n",
       "14                clothing  0.983101  0.983791  0.986904\n",
       "15                   money  0.970185  0.971153  0.978767\n",
       "16          missing_people  0.983112  0.988688  0.988557\n",
       "17                refugees  0.955561  0.955685  0.967451\n",
       "18                   death  0.951104  0.956573  0.961602\n",
       "19               other_aid   0.84216  0.840074  0.871329\n",
       "20  infrastructure_related  0.909023   0.91061  0.935919\n",
       "21               transport  0.941023   0.94704  0.956771\n",
       "22               buildings  0.945112   0.94529  0.955372\n",
       "23             electricity  0.970561  0.971502  0.979021\n",
       "24                   tools  0.990791   0.98783   0.99377\n",
       "25               hospitals  0.985149  0.980264  0.990083\n",
       "26                   shops  0.994663  0.992893   0.99644\n",
       "27             aid_centers  0.980336  0.973978  0.986777\n",
       "28    other_infrastructure  0.936331  0.930149  0.956008\n",
       "29         weather_related  0.872235  0.871761  0.872854\n",
       "30                  floods  0.935705  0.938453  0.944437\n",
       "31                   storm  0.934339  0.932969  0.936427\n",
       "32                    fire   0.98584  0.987665  0.989955\n",
       "33              earthquake  0.968222  0.967977  0.969104\n",
       "34                    cold  0.971675  0.971777  0.978894\n",
       "35           other_weather  0.932659  0.933459  0.949015\n",
       "36           direct_report  0.835957  0.832709  0.841831"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting using the RF tuned model \n",
    "get_results(Y_test, cv_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated f_score: 0.938035264673\n",
      "Aggregated precision: 0.938657791605\n",
      "Aggregated recall: 0.946701278519\n",
      "Accuracy: 0.946701278519\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>f_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>related</td>\n",
       "      <td>0.673748</td>\n",
       "      <td>0.738787</td>\n",
       "      <td>0.760839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>request</td>\n",
       "      <td>0.884464</td>\n",
       "      <td>0.885127</td>\n",
       "      <td>0.892689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer</td>\n",
       "      <td>0.99244</td>\n",
       "      <td>0.99086</td>\n",
       "      <td>0.994024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aid_related</td>\n",
       "      <td>0.759308</td>\n",
       "      <td>0.762157</td>\n",
       "      <td>0.763382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical_help</td>\n",
       "      <td>0.911946</td>\n",
       "      <td>0.908831</td>\n",
       "      <td>0.924348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical_products</td>\n",
       "      <td>0.946993</td>\n",
       "      <td>0.946851</td>\n",
       "      <td>0.955245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>search_and_rescue</td>\n",
       "      <td>0.96883</td>\n",
       "      <td>0.966879</td>\n",
       "      <td>0.975334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>security</td>\n",
       "      <td>0.972801</td>\n",
       "      <td>0.968191</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>military</td>\n",
       "      <td>0.968345</td>\n",
       "      <td>0.966818</td>\n",
       "      <td>0.971901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>child_alone</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>water</td>\n",
       "      <td>0.96207</td>\n",
       "      <td>0.961417</td>\n",
       "      <td>0.963001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>food</td>\n",
       "      <td>0.944621</td>\n",
       "      <td>0.944084</td>\n",
       "      <td>0.946853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shelter</td>\n",
       "      <td>0.939768</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.944056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clothing</td>\n",
       "      <td>0.985953</td>\n",
       "      <td>0.985618</td>\n",
       "      <td>0.987921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>money</td>\n",
       "      <td>0.974691</td>\n",
       "      <td>0.972981</td>\n",
       "      <td>0.977368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>missing_people</td>\n",
       "      <td>0.984398</td>\n",
       "      <td>0.98289</td>\n",
       "      <td>0.98754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>refugees</td>\n",
       "      <td>0.960493</td>\n",
       "      <td>0.958065</td>\n",
       "      <td>0.967069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>death</td>\n",
       "      <td>0.959146</td>\n",
       "      <td>0.959706</td>\n",
       "      <td>0.964908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other_aid</td>\n",
       "      <td>0.838275</td>\n",
       "      <td>0.836305</td>\n",
       "      <td>0.870057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>infrastructure_related</td>\n",
       "      <td>0.913773</td>\n",
       "      <td>0.908588</td>\n",
       "      <td>0.933884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>transport</td>\n",
       "      <td>0.946353</td>\n",
       "      <td>0.945596</td>\n",
       "      <td>0.956898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>buildings</td>\n",
       "      <td>0.955423</td>\n",
       "      <td>0.954851</td>\n",
       "      <td>0.960966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>electricity</td>\n",
       "      <td>0.975569</td>\n",
       "      <td>0.974753</td>\n",
       "      <td>0.980038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tools</td>\n",
       "      <td>0.990219</td>\n",
       "      <td>0.987824</td>\n",
       "      <td>0.992626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hospitals</td>\n",
       "      <td>0.985548</td>\n",
       "      <td>0.983471</td>\n",
       "      <td>0.988557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>shops</td>\n",
       "      <td>0.994027</td>\n",
       "      <td>0.992888</td>\n",
       "      <td>0.995168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aid_centers</td>\n",
       "      <td>0.980648</td>\n",
       "      <td>0.977601</td>\n",
       "      <td>0.984997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>other_infrastructure</td>\n",
       "      <td>0.938905</td>\n",
       "      <td>0.931777</td>\n",
       "      <td>0.952448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>weather_related</td>\n",
       "      <td>0.871006</td>\n",
       "      <td>0.873795</td>\n",
       "      <td>0.875906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>floods</td>\n",
       "      <td>0.955452</td>\n",
       "      <td>0.955883</td>\n",
       "      <td>0.958805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>storm</td>\n",
       "      <td>0.934788</td>\n",
       "      <td>0.933863</td>\n",
       "      <td>0.939352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fire</td>\n",
       "      <td>0.987199</td>\n",
       "      <td>0.986229</td>\n",
       "      <td>0.989193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.97146</td>\n",
       "      <td>0.971218</td>\n",
       "      <td>0.971901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cold</td>\n",
       "      <td>0.979085</td>\n",
       "      <td>0.97922</td>\n",
       "      <td>0.982454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>other_weather</td>\n",
       "      <td>0.932035</td>\n",
       "      <td>0.927146</td>\n",
       "      <td>0.945073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>direct_report</td>\n",
       "      <td>0.82949</td>\n",
       "      <td>0.832314</td>\n",
       "      <td>0.846917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Category   f_score precision    recall\n",
       "1                  related  0.673748  0.738787  0.760839\n",
       "2                  request  0.884464  0.885127  0.892689\n",
       "3                    offer   0.99244   0.99086  0.994024\n",
       "4              aid_related  0.759308  0.762157  0.763382\n",
       "5             medical_help  0.911946  0.908831  0.924348\n",
       "6         medical_products  0.946993  0.946851  0.955245\n",
       "7        search_and_rescue   0.96883  0.966879  0.975334\n",
       "8                 security  0.972801  0.968191   0.97953\n",
       "9                 military  0.968345  0.966818  0.971901\n",
       "10             child_alone         1         1         1\n",
       "11                   water   0.96207  0.961417  0.963001\n",
       "12                    food  0.944621  0.944084  0.946853\n",
       "13                 shelter  0.939768    0.9391  0.944056\n",
       "14                clothing  0.985953  0.985618  0.987921\n",
       "15                   money  0.974691  0.972981  0.977368\n",
       "16          missing_people  0.984398   0.98289   0.98754\n",
       "17                refugees  0.960493  0.958065  0.967069\n",
       "18                   death  0.959146  0.959706  0.964908\n",
       "19               other_aid  0.838275  0.836305  0.870057\n",
       "20  infrastructure_related  0.913773  0.908588  0.933884\n",
       "21               transport  0.946353  0.945596  0.956898\n",
       "22               buildings  0.955423  0.954851  0.960966\n",
       "23             electricity  0.975569  0.974753  0.980038\n",
       "24                   tools  0.990219  0.987824  0.992626\n",
       "25               hospitals  0.985548  0.983471  0.988557\n",
       "26                   shops  0.994027  0.992888  0.995168\n",
       "27             aid_centers  0.980648  0.977601  0.984997\n",
       "28    other_infrastructure  0.938905  0.931777  0.952448\n",
       "29         weather_related  0.871006  0.873795  0.875906\n",
       "30                  floods  0.955452  0.955883  0.958805\n",
       "31                   storm  0.934788  0.933863  0.939352\n",
       "32                    fire  0.987199  0.986229  0.989193\n",
       "33              earthquake   0.97146  0.971218  0.971901\n",
       "34                    cold  0.979085   0.97922  0.982454\n",
       "35           other_weather  0.932035  0.927146  0.945073\n",
       "36           direct_report   0.82949  0.832314  0.846917"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting using the ADA tuned model \n",
    "get_results(y_test, cv_ADA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have already used two classifiers and applied Gridsearch to select the best between them. Hence We would be using Random forest as the final classifier as this shows us the best result in terms of F1 score , precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pickle file for the model\n",
    "pkl_file = 'final_model.pkl'\n",
    "with open (pkl_file, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,label_ranking_average_precision_score\n",
    "from sklearn.model_selection  import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "def load_data(database_filepath):\n",
    "    \"\"\"\n",
    "       Function:\n",
    "       load data from database\n",
    "       Args:\n",
    "       database_filepath: the path of the database\n",
    "       Return:\n",
    "       X (DataFrame) : Message features dataframe\n",
    "       Y (DataFrame) : target dataframe\n",
    "       category (list of str) : target labels list\n",
    "       \"\"\"\n",
    "    engine = create_engine('sqlite:///{}'.format(database_filepath))\n",
    "    df = pd.read_sql_table('SELECT * from DisasterResponse', engine)\n",
    "    X = df['message']  # Message Column\n",
    "    Y = df.iloc[:, 4:]  # Classification label\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function: tokenize the text\n",
    "    Args:  source string\n",
    "    Return:\n",
    "    clean_tokens(str list): clean string list\n",
    "    \n",
    "    \"\"\"\n",
    "    #normalize text\n",
    "    text = re.sub(r'[^a-zA-Z0-9]',' ',text.lower())\n",
    "    \n",
    "    #token messages\n",
    "    words = word_tokenize(text)\n",
    "    tokens = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    #sterm and lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    \"\"\"\n",
    "    Pipleine 1: Random Forest Classifier\n",
    "    \"\"\"\n",
    "    pipeline_RF = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf',  MultiOutputClassifier(RandomForestClassifier(class_weight='balanced')))\n",
    "      ])\n",
    "    \n",
    "    parameters_RF = {\n",
    "        'vect__max_df': [1.0,2.0],\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'clf__estimator__criterion': ['gini'],\n",
    "        'clf__estimator__n_estimators': [10,20],\n",
    "        'clf__estimator__min_samples_split':[2,5]\n",
    "        }\n",
    "\n",
    "    cv_RF = GridSearchCV(pipeline_RF, param_grid = parameters_RF, verbose=True,cv=3,n_jobs = -1)\n",
    "    return cv_RF\n",
    "\n",
    "\n",
    "\n",
    "def get_results(Y_test, model, X_test):\n",
    "    y_pred=model.predict(X_test)\n",
    "    report = pd.DataFrame(columns=['Category', 'f_score', 'precision', 'recall'])\n",
    "    num = 0\n",
    "    for colnm in Y_test.columns:\n",
    "        precision, recall, f_score, support = precision_recall_fscore_support(Y_test[colnm], y_pred[:,num], average='weighted')\n",
    "        report.at[num+1, 'Category'] = colnm\n",
    "        report.at[num+1, 'f_score'] = f_score\n",
    "        report.at[num+1, 'precision'] = precision\n",
    "        report.at[num+1, 'recall'] = recall\n",
    "        num += 1\n",
    "    print('Aggregated f_score:', report['f_score'].mean())\n",
    "    print('Aggregated precision:', report['precision'].mean())\n",
    "    print('Aggregated recall:', report['recall'].mean())\n",
    "    print('Accuracy:', np.mean(Y_test.values == y_pred))\n",
    "    return report   \n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    \n",
    "    # Create a pickle file for the model\n",
    "    with open (model_filepath, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) == 3:\n",
    "        database_filepath, model_filepath = sys.argv[1:]\n",
    "        print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        X, Y, category_names = load_data(database_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "        \n",
    "        print('Building model...')\n",
    "        model = build_model()\n",
    "        \n",
    "        print('Training model...')\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        print('Evaluating model...')\n",
    "        get_results(Y_test, model, X_test)\n",
    "\n",
    "        print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('Trained model saved!')\n",
    "\n",
    "    else:\n",
    "        print('Please provide the filepath of the disaster messages database '\\\n",
    "              'as the first argument and the filepath of the pickle file to '\\\n",
    "              'save the model to as the second argument. \\n\\nExample: python '\\\n",
    "              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
